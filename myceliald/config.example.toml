[node]
display_name = "Daemon0" # Display name of the daemon in the Mycelial UI.
unique_id = "daemon0" # Unique identifier for the daemon. This should be unique across all daemons.
storage_path = "client.db" # Path to the SQLite database file that the daemon will use to store state.
auth_token = "d135801c-fd73-477c-b0a8-055d0d117485" # Auth token that the daemon will use to authenticate with the Mycelial control plane.

[server]
endpoint = "http://localhost:7777" # URL of the Mycelial control plane.

[[sources]]
# Directory source 
type = "dir" 
# This source will read all files in the specified directory. Files will be synced only once and will not be monitored for changes.
display_name = "Directory Source"
# The display_name field specifies the name of the source in the Mycelial UI.
path = "/path/to/directory/**/*.txt"
# Only absulute paths are supported. Supports regular expressions.
start_after = ""
# The start_after field specifies the file name to start reading from. If this field is empty, the daemon will read from the beginning of the directory.
interval = 5
# The interval field specifies how often the daemon should check for new files in seconds.
stream_binary = false
# if true - files will be streamed directly by dir, otherwise section sends dataframe with path to file

[[sources]]
# Excel workbook source
type = "excel_connector"
# This source will read data from an Excel workbook.
display_name = "Excel Source"
# The display_name field specifies the name of the source in the Mycelial UI.
path = "/path/to/your/excel.xlsx"
# The path field specifies the path to the Excel workbook. Only absolute paths are supported.
sheets = "*"
# The sheets field specifies the sheets to read from. This field is optional and defaults to all sheets.
# 
strict = false
# Allows mixed data types in Excel columns.
# If set to false, the daemon will convert all data to strings.
# If set to true, the daemon will ignore any rows where the datatype does not match the datatype of the first row for that column.

[[sources]]
# Exec commmand source
# NOTE: Binary streams are not yet supported.
type = "exec"
# This source will execute the specified command and read the output.
display_name = "Exec Source"
# The display_name field specifies the name of the source in the Mycelial UI.
command = "echo"
# The command field specifies the command to execute.
args = "$foo $baz"
# The args field specifies the arguments to pass to the command. This field is optional.
row_as_args = false
# If set to true, incoming rows will be passed as arguments to the command.
# For example, given the dataframe below, the command  will receive 2 extra arguments as --col=val --col1=val1
# +------------+
# | col | col1 |
# +-----+------+
# | val | val1 |
# +-----+------+
ack_passthrough = false
# If set to true, section will not ack processed rows, delegating message acknowledgement to a downstream section.
env = "foo=bar, baz=qux"
# The env field specifies the environment variables to set when executing the command. This field is optional.


[[sources]]
# File source
type = "file"
# This source will read the specified file and will watch the file for changes.
display_name = "File Source"
# The display_name field specifies the name of the source in the Mycelial UI.
path = "/path/to/your/file.extension"
# The path field specifies the path to the file. Only absolute paths are supported.
interval = 5
# The interval field specifies how often the daemon should check for changes in the file in seconds.

[[sources]]
# Debug source
type = "hello_world"
# This source will emit a message at a given interval. It is intended for testing and debugging purposes.
display_name = "Debug Source"
# The display_name field specifies the name of the source in the Mycelial UI.
interval_milis = 5000
# The interval_milis field specifies how often the source should emit a message in milliseconds.
message = "Hello World"
# The message field specifies the message to emit.

[[sources]]
# MySQL source
type = "mysql_connector"
# This source will read data from a MySQL database.
display_name = "MySQL Source"
# The display_name field specifies the name of the source in the Mycelial UI.
origin = "example"
# The origin field specifies the origin of the data. Downstream destination tables created with data from this query will be named this value.
url = "mysql://username:password@127.0.0.1:3306/example"
# The url field specifies the connection string to the MySQL database.
query = "select * from example;"
# The query field specifies the query to execute.
poll_interval = 5
# The poll_interval field specifies how often the daemon should poll the database for new data in seconds.

[[sources]]
type = "postgres_connector"
# This source will read data from a Postgres database.
display_name = "Postgres Source"
# The display_name field specifies the name of the source in the Mycelial UI.
url = "postgres://user:password@localhost:5432/test"
# The url field specifies the connection string to the Postgres database.
origin = "example"
# The origin field specifies the origin of the data. Downstream destination tables created with data from this query will be named this value.
query = "select * from test;"
# The query field specifies the query to execute.
poll_interval = 5
# The poll_interval field specifies how often the daemon should poll the database for new data in seconds.

[[sources]]
type = "snowflake"
# This source will read data from a Snowflake database.
display_name = "Snowflake Source"
# The display_name field specifies the name of the source in the Mycelial UI.
username = "username"
# The username field specifies the username to use when connecting to the Snowflake database.
password = "password"
# The password field specifies the password to use when connecting to the Snowflake database.
role = "role"
# The role field specifies the role to use when connecting to the Snowflake database.
account_identifier = "account_identifier"
# The account_identifier field specifies the account identifier to use when connecting to the Snowflake database.
warehouse = "warehouse"
# The warehouse field specifies the data warehouse to use when connecting to the Snowflake database.
database = "database"
# The database field specifies the database to use when connecting to the Snowflake database.
schema = "schema"
# The schema field specifies the schema to use when connecting to the Snowflake database.
query = "query"
# The query field specifies the query to execute.
delay = 5
# The delay field specifies how long to wait before executing the query in seconds.

[[sources]]
# SQLite source
type = "sqlite_connector"
# This source will read data from a SQLite database.
display_name = "Sqlite Source"
# The display_name field specifies the name of the source in the Mycelial UI.
path = "/path/to/your/database.sqlite"
# The path field specifies the path to the SQLite database. Only absolute paths are supported.
origin = "SQLite"
# The origin field specifies the origin of the data. Downstream destination tables created with data from this query will be named this value.
query = "select * from example;"
# The query field specifies the query to execute.

[[sources]]
# Tagging Transformer
type = "tagging_transformer"
# This source will add a column to the dataframe with the specified value.
display_name = "Tagging Transformer"
# The display_name field specifies the name of the source in the Mycelial UI.
column = "tag"
# The column field specifies the name of the column to add.
text = "value"
# The text field specifies the value to add to the column.

[[sources]]
# Typecast Transformer
type = "typecast_transformer"
# This source will cast the specified column to the specified type.
display_name = "Typecast Transformer"
# The display_name field specifies the name of the source in the Mycelial UI.
from = "integer"
# The from field specifies the current type of the column.
target_type = "string"
# The target_type field specifies the type to cast the column to.
column = "column_name"
# The column field specifies the name of the column to cast.

[[sources]]
# Converts binary csv stream into stream of dataframes
type = "csv"
display_name = "from_csv"
# max length of dataframe
batch_size = 512

[[sources]]
# Transforms message origin
type = "origin_transform"
display_name = "origin_transform"
# max length of dataframe
regex = ""
replacement = ""

[[destinations]]
# File destination
type = "file"
# This destination will write the data to the specified file.
display_name = "File Destination"
# The display_name field specifies the name of the destination in the Mycelial UI.
dir_path = "/tmp"
# The path field specifies the dir path to where file will be put

[[destinations]]
# Debug Destination
type = "hello_world"
# This destination will print the dataframe to the console.
display_name = "Debug Destination"
# The display_name field specifies the name of the destination in the Mycelial UI.

[[destinations]]
# Kafka Destination
type = "kafka"
# This destination will write each row in a dataframe as a message to the Kafka topic specified.
# Incoming dataframe rows are automatically converted to JSON.
display_name = "Kafka Destination"
# The display_name field specifies the name of the destination in the Mycelial UI.
brokers = "localhost:9092"
# The brokers field specifies the Kafka brokers to connect to.
topic = "example_topic"
# The topic field specifies the Kafka topic to write to.

[[destinations]]
# MySQL Destination
type = "mysql_connector"
# This destination will write the data to a MySQL database.
display_name = "MySQL Destination"
# The display_name field specifies the name of the destination in the Mycelial UI.
url = "mysql://username:password@127.0.0.1:3306/test"
# The url field specifies the connection string to the MySQL database.
truncate = true
# The truncate field specifies whether to truncate the destination table before writing the data.

[[destinations]]
# Postgres Destination
type = "postgres_connector"
# This destination will write the data to a Postgres database. 
# Data will be written to the specified schema into a table named after the Origin field of the data source.
# If the table does not exist, it will be created.
display_name = "Postgres Destination"
# The display_name field specifies the name of the destination in the Mycelial UI.
url = "postgres://user:password@127.0.0.1:5432/example"
# The url field specifies the connection string to the Postgres database.
schema = "example"
# The schema field specifies the schema to write to.
truncate = true 
# The truncate field specifies whether to truncate the destination table before writing the data.

[[destinations]]
# Snowflake Destination
type = "snowflake"
# This destination will write the data to a Snowflake database.
display_name = "Snowflake Destination"
# The display_name field specifies the name of the destination in the Mycelial UI.
username = "username"
# The username field specifies the username to use when connecting to the Snowflake database.
password = "password"
# The password field specifies the password to use when connecting to the Snowflake database.
role = "role"
# The role field specifies the role to use when connecting to the Snowflake database.
account_identifier = "account_identifier"
# The account_identifier field specifies the account identifier to use when connecting to the Snowflake database.
warehouse = "warehouse"
# The warehouse field specifies the data warehouse to use when connecting to the Snowflake database.
database = "database"
# The database field specifies the database to use when connecting to the Snowflake database.
schema = "schema"
# The schema field specifies the schema to use when connecting to the Snowflake database.
truncate = true
# The truncate field specifies whether to truncate the destination table before writing the data.

[[destinations]]
# SQLite Destination
type = "sqlite_connector"
# This destination will write the data to a SQLite database.
display_name = "Sqlite Destination"
# The display_name field specifies the name of the destination in the Mycelial UI.
path = "/path/to/your/dest_database.sqlite"
# The path field specifies the path to the SQLite database. Only absolute paths are supported.
truncate = true 
# The truncate field specifies whether to truncate the destination table before writing the data.

